# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17fp8h00eqWDZvRiE1UodQv_ISGTrnbHF
"""

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments
from fastapi import FastAPI
from pydantic import BaseModel
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load the GPT-2 tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Fine-tune the model on a specific dataset
train_dataset = TextDataset(tokenizer=tokenizer, file_path='https://raw.githubusercontent.com/sonyalomsadze/myapp/main/shakespear.txt', block_size=128)
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
training_args = TrainingArguments(
    output_dir='./results',
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=16,
    save_steps=10_000,
    save_total_limit=2,
    prediction_loss_only=True,
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator,
)
trainer.train()
# Generate text using the fine-tuned model
input_ids = tokenizer.encode("KING LEAR:", return_tensors='pt')
attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=model.device)
pad_token_id = tokenizer.eos_token_id
generated_text = model.generate(input_ids, max_length=100, do_sample=True, temperature=0.7, attention_mask=attention_mask, pad_token_id=pad_token_id)
generated_text = tokenizer.decode(generated_text[0], skip_special_tokens=True)
print(generated_text)


model.save_pretrained("saved_model")
tokenizer.save_pretrained('my_tokenizer')


model_path = "saved_model"
tokenizer_path = "my_tokenizer"
max_length = 100
temperature = 0.7

app = FastAPI()

# Load the model and tokenizer
model = GPT2LMHeadModel.from_pretrained(model_path)
tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)

# Define the request body schema
class GenerateTextRequest(BaseModel):
    prompt: str

# Define the response body schema
class GenerateTextResponse(BaseModel):
    text: str

# Define the POST endpoint for generating text
@app.post("/generate_text", response_model=GenerateTextResponse)
async def generate_text(request: GenerateTextRequest):
    # Generate text using the provided prompt
    input_ids = tokenizer.encode(request.prompt, return_tensors="pt")
    generated_ids = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=temperature)
    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

    # Return the generated text as the response
    return GenerateTextResponse(text=generated_text)

#!uvicorn app:app --reload
